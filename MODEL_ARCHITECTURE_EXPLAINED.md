# ğŸ§  Complete Model Architecture Explanation

## ğŸ“Š **Overview: 5 Models Working Together**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FACE SWAPPING PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”â”‚
â”‚  â”‚ Input   â”‚â”€â”€â”€>â”‚ Model  â”‚â”€â”€â”€>â”‚ Model   â”‚â”€â”€â”€>â”‚ Model   â”‚â”€â”€â”€>â”‚Outâ”‚â”‚
â”‚  â”‚ Images  â”‚    â”‚ 1,2,3  â”‚    â”‚   4     â”‚    â”‚   5     â”‚    â”‚putâ”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”˜â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ **The 5 Models Used**

### **MODEL 1: SCRFD (Face Detection)**
- **File**: `insightface_func/models/antelope/scrfd_10g_bnkps.onnx`
- **Type**: ONNX Runtime model
- **Purpose**: Find faces in images/video frames
- **Input**: RGB image (any size)
- **Output**: 
  - Bounding boxes `[x, y, w, h, confidence]`
  - 5 facial keypoints (eyes, nose, mouth corners)

```python
# Where it's loaded (inference.py line 22-23):
app = Face_detect_crop(name='antelope', root='./insightface_func/models')
app.prepare(ctx_id=0, det_thresh=0.6, det_size=(640,640))
```

**Speed**: ~20ms per frame @ 640x640  
**VRAM**: ~300MB

---

### **MODEL 2: GLintr100 (Face Recognition)**
- **File**: `insightface_func/models/antelope/glintr100.onnx`
- **Type**: ONNX Runtime model (InsightFace)
- **Purpose**: Extract face embeddings for matching source/target
- **Input**: Aligned face (112x112)
- **Output**: 512-dimensional embedding vector

```python
# Loaded automatically by InsightFace app
# Used to match which face in target video to swap
```

**Purpose**: Ensures you swap the right person (not everyone in the frame)  
**Speed**: ~15ms per face  
**VRAM**: ~200MB

---

### **MODEL 3: ArcFace (iResNet100)**
- **File**: `arcface_model/backbone.pth`
- **Type**: PyTorch model
- **Architecture**: Improved ResNet-100
- **Purpose**: Extract deep identity features for face swapping
- **Input**: Aligned face (112x112)
- **Output**: 512-dimensional identity embedding

```python
# Where it's loaded (inference.py lines 33-36):
netArc = iresnet100(fp16=False)
netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))
netArc = netArc.cuda()
netArc.eval()
```

**Key Difference from Model 2**: 
- Model 2 (GLintr100) = Quick matching/recognition
- Model 3 (ArcFace) = Deep identity encoding for generation

**Speed**: ~50ms per face  
**VRAM**: ~400MB

---

### **MODEL 4: AEI-Net (Main Generator) â­ CORE MODEL**
- **File**: `weights/G_unet_2blocks.pth`
- **Type**: PyTorch model (Custom architecture)
- **Architecture**: 
  - **Encoder**: U-Net with 7 conv layers â†’ Multi-level attributes
  - **Generator**: AAD (Adaptive Attentional Denormalization) blocks
- **Purpose**: Generate swapped face with source identity + target attributes
- **Input**: 
  - Target face (256x256 RGB)
  - Source identity embedding (512-dim from ArcFace)
- **Output**: Swapped face (256x256 RGB)

```python
# Where it's loaded (inference.py lines 26-30):
G = AEI_Net(args.backbone, num_blocks=args.num_blocks, c_id=512)
G.eval()
G.load_state_dict(torch.load(args.G_path, map_location=torch.device('cpu')))
G = G.cuda()
G = G.half()  # FP16 for speed
```

**Architecture Details**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AEI-Net Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Target Face (256x256)          Source Identity (512-dim)  â”‚
â”‚         â”‚                                â”‚                  â”‚
â”‚         â–¼                                â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚                  â”‚
â”‚  â”‚   Encoder    â”‚                       â”‚                  â”‚
â”‚  â”‚   (U-Net)    â”‚                       â”‚                  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                       â”‚                  â”‚
â”‚  â”‚ Conv 3â†’32    â”‚                       â”‚                  â”‚
â”‚  â”‚ Conv 32â†’64   â”‚                       â”‚                  â”‚
â”‚  â”‚ Conv 64â†’128  â”‚                       â”‚                  â”‚
â”‚  â”‚ Conv 128â†’256 â”‚â”€â”€â”€â”                   â”‚                  â”‚
â”‚  â”‚ Conv 256â†’512 â”‚   â”‚  Skip Connections â”‚                  â”‚
â”‚  â”‚ Conv 512â†’1024â”‚   â”‚                   â”‚                  â”‚
â”‚  â”‚ Conv 1024â†’1024   â”‚                   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                   â”‚                  â”‚
â”‚         â”‚            â”‚                   â”‚                  â”‚
â”‚         â”‚            â”‚                   â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚             AAD Generator                            â”‚  â”‚
â”‚  â”‚  (Adaptive Attentional Denormalization)              â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  8 AAD Blocks (with 2 ResBlocks each)               â”‚  â”‚
â”‚  â”‚  - Inject source identity at each level              â”‚  â”‚
â”‚  â”‚  - Preserve target attributes from encoder           â”‚  â”‚
â”‚  â”‚  - Adaptive normalization for blending               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚                  Swapped Face (256x256)                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What makes it special**:
- **AAD Blocks**: Adaptively blend source identity with target attributes
- **Multi-level features**: Preserves facial structure while changing identity
- **Skip connections**: Maintains fine details (hair, background, expressions)

**Speed**: ~120ms per face (BOTTLENECK!)  
**VRAM**: ~2GB  
**Parameters**: ~54 million

---

### **MODEL 5: 2D106Det (Facial Landmarks) [OPTIONAL]**
- **File**: `coordinate_reg/model/2d106det-0000.onnx`
- **Type**: ONNX Runtime model
- **Purpose**: Detect 106 facial landmarks for better alignment
- **Input**: Face crop (192x192)
- **Output**: 106 (x, y) landmark coordinates

```python
# Where it's loaded (inference.py line 39):
handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)
```

**Note**: Your current setup falls back to InsightFace's 5-point landmarks since this ONNX file is not present.

**Speed**: ~10ms per face  
**VRAM**: ~150MB

---

### **MODEL 6: Pix2Pix (Super Resolution) [OPTIONAL]**
- **File**: `weights/10_net_G.pth`
- **Type**: PyTorch model (Pix2Pix GAN)
- **Purpose**: Enhance swapped face quality (upscale/denoise)
- **Input**: Swapped face (256x256)
- **Output**: Enhanced face (256x256, higher quality)

```python
# Where it's loaded (inference.py lines 42-50):
if args.use_sr:
    model = Pix2PixModel(opt)
    model.netG.train()
```

**Default**: Disabled (`use_sr=False`)  
**Speed**: +80ms per face  
**VRAM**: +1GB

---

## ğŸ”„ **Complete Pipeline Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STEP-BY-STEP PROCESS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: source.jpg (Mark Zuckerberg) + target.jpg (David Beckham)

STEP 1: Load Source Face
â”œâ”€ Read source.jpg
â”œâ”€ MODEL 1 (SCRFD): Detect face â†’ bbox + 5 keypoints
â”œâ”€ Crop and align face to 224x224
â””â”€ Store as source_crop

STEP 2: Extract Source Identity
â”œâ”€ Resize source_crop to 112x112
â”œâ”€ MODEL 3 (ArcFace): source_crop â†’ source_embedding [512-dim]
â””â”€ Store source_embedding (this IS the person's identity!)

STEP 3: Load Target Face
â”œâ”€ Read target.jpg
â”œâ”€ MODEL 1 (SCRFD): Detect face â†’ bbox + 5 keypoints
â”œâ”€ Crop and align face to 224x224
â”œâ”€ Store as target_crop
â””â”€ Store transformation matrix M (for blending back later)

STEP 4: Extract Target Identity (for matching)
â”œâ”€ Resize target_crop to 112x112
â”œâ”€ MODEL 3 (ArcFace): target_crop â†’ target_embedding [512-dim]
â”œâ”€ Compare with source_embedding (cosine similarity)
â””â”€ If similarity < threshold â†’ skip this face (not the target person)

STEP 5: Generate Swapped Face â­ MAGIC HAPPENS HERE
â”œâ”€ Resize target_crop to 256x256
â”œâ”€ MODEL 4 (AEI-Net): 
â”‚   Input 1: target_crop (256x256) - provides attributes (pose, expression, lighting)
â”‚   Input 2: source_embedding (512-dim) - provides identity (who it looks like)
â”‚   â”œâ”€ Encoder extracts multi-level features from target
â”‚   â””â”€ Generator blends source identity into target attributes
â”‚   Output: swapped_face (256x256) - Mark's face with Beckham's pose/expression
â””â”€ Store swapped_face

STEP 6: Optional Enhancement
â””â”€ If use_sr=True:
    MODEL 6 (Pix2Pix): swapped_face â†’ enhanced_face

STEP 7: Blend Back into Original Image
â”œâ”€ MODEL 5 (or fallback): Detect landmarks on swapped_face
â”œâ”€ Generate face mask (smooth edges)
â”œâ”€ Use transformation matrix M to warp swapped_face back
â”œâ”€ Alpha blend with original target.jpg
â””â”€ Result: Beckham's photo but with Mark's face!

OUTPUT: result.png
```

---

## âš™ï¸ **Model Configuration**

### **Current Settings** (from inference.py):

```python
# Generator settings:
backbone='unet'           # Architecture type (unet | linknet | resnet)
num_blocks=2              # Number of AAD ResBlocks (1, 2, or 3)
G_path='weights/G_unet_2blocks.pth'  # Trained weights

# Face detection settings:
det_thresh=0.6            # Minimum confidence to detect face
det_size=(640, 640)       # Detection resolution
crop_size=224             # Face crop size

# Face matching settings:
similarity_th=0.15        # Cosine distance threshold (lower = stricter match)

# Performance settings:
batch_size=40             # Process 40 frames at once (for videos)
use_sr=False              # Super resolution disabled (for speed)
```

---

## ğŸ’¾ **Model Sizes & Weights**

```
Total Model Files: ~1.5 GB

â”œâ”€ scrfd_10g_bnkps.onnx           â†’  16 MB
â”œâ”€ glintr100.onnx                 â†’  260 MB
â”œâ”€ backbone.pth (ArcFace)         â†’  249 MB
â”œâ”€ G_unet_2blocks.pth (AEI-Net)   â†’ 210 MB
â”œâ”€ 2d106det-0000.onnx             â†’  5 MB (optional)
â””â”€ 10_net_G.pth (Pix2Pix)         â†’ 800 MB (optional)
```

---

## ğŸš€ **Performance Breakdown**

### **Timing per Frame** (Single face @ 640x640 input):

```
Total: ~230ms per frame = 4.3 FPS

â”œâ”€ Face Detection (SCRFD)         â†’  20ms  (9%)
â”œâ”€ Face Alignment                 â†’   5ms  (2%)
â”œâ”€ Source Embedding (ArcFace)     â†’  50ms  (22%) [Once per source]
â”œâ”€ Target Embedding (ArcFace)     â†’  50ms  (22%)
â”œâ”€ Face Swap Generation (AEI-Net) â†’ 120ms  (52%) âš ï¸ BOTTLENECK
â”œâ”€ Landmark Detection             â†’  10ms  (4%)
â””â”€ Blending/Warping               â†’  15ms  (7%)
```

### **GPU Memory Usage** (Colab T4):

```
Total: ~3.2 GB / 16 GB available

â”œâ”€ SCRFD                â†’  0.3 GB
â”œâ”€ GLintr100            â†’  0.2 GB
â”œâ”€ ArcFace              â†’  0.4 GB
â”œâ”€ AEI-Net (Generator)  â†’  2.0 GB âš ï¸ LARGEST
â”œâ”€ 2D106Det             â†’  0.15 GB
â””â”€ PyTorch overhead     â†’  0.15 GB
```

---

## ğŸ”¬ **Technical Details**

### **What Each Model Actually Does**:

| Model | Task | Input Dimension | Output Dimension | Purpose |
|-------|------|----------------|------------------|---------|
| **SCRFD** | Detection | (H, W, 3) | Nx5x2 + Nx5 | "Where are the faces?" |
| **GLintr100** | Recognition | (112, 112, 3) | 512-dim | "Is this the target person?" |
| **ArcFace** | Embedding | (112, 112, 3) | 512-dim | "What defines this person's identity?" |
| **AEI-Net** | Generation | (256, 256, 3) + 512-dim | (256, 256, 3) | "Swap the identity!" |
| **2D106Det** | Landmarks | (192, 192, 3) | 106x2 | "Where are facial features?" |
| **Pix2Pix** | Enhancement | (256, 256, 3) | (256, 256, 3) | "Make it look better!" |

---

## ğŸ§ª **Model Variants Available**

### **Generator Weights** (choose one):

```python
# Different configurations of AEI-Net:

'weights/G_unet_1block.pth'    # Faster, less quality
'weights/G_unet_2blocks.pth'   # Balanced (DEFAULT) â­
'weights/G_unet_3blocks.pth'   # Slower, best quality

'weights/G_linknet_2blocks.pth' # Alternative architecture
```

**Trade-off**:
- 1 block: ~90ms per face, 7/10 quality
- 2 blocks: ~120ms per face, 8.5/10 quality â­
- 3 blocks: ~150ms per face, 9/10 quality

---

## ğŸ¯ **Key Insights**

### **1. The Core Innovation: AEI-Net**

The magic is in the **AAD (Adaptive Attentional Denormalization)** blocks:

```python
# Traditional approach (bad):
output = source_features + target_features  # Simple addition

# AAD approach (good):
output = AAD(source_identity, target_attributes)
# Adaptively decides:
# - Which source features to keep (identity: eyes, nose, face shape)
# - Which target features to keep (attributes: pose, expression, lighting)
```

**Result**: Realistic face swap that preserves:
- âœ… Source person's identity
- âœ… Target person's pose/expression
- âœ… Target scene's lighting/angle
- âœ… Natural transitions (no hard edges)

### **2. Why Two Face Recognition Models?**

**GLintr100** (ONNX):
- Fast, lightweight
- Used for matching ("which face in the video?")
- Runs on CPU if needed

**ArcFace** (PyTorch):
- Slower but more detailed
- Used for generation ("extract deep identity")
- Needs GPU for real-time

### **3. Bottleneck Analysis**

For real-time (30 FPS = 33ms budget):

```
Current: 230ms â†’ 4.3 FPS âŒ

Options to reach real-time:
â”œâ”€ Reduce detection size (640â†’320): Save 12ms
â”œâ”€ Skip detection (use tracking): Save 15ms  
â”œâ”€ Lighter generator (1 block): Save 30ms
â”œâ”€ Lower resolution (256â†’128): Save 80ms âš ï¸ Quality loss
â””â”€ Model quantization (FP16â†’INT8): Save 40ms
```

**Best combination for real-time**:
- Detection every 3-5 frames + tracking
- Keep 2-block generator (quality matters)
- Optimize blending pipeline
â†’ Target: 12-15 FPS (acceptable for demos)

---

## ğŸ“š **Model Papers & References**

1. **SCRFD** (2021): "Sample and Computation Redistribution for Efficient Face Detection"
2. **ArcFace** (2019): "ArcFace: Additive Angular Margin Loss for Deep Face Recognition"
3. **AEI-Net** (2020): "Face Swapping via Adaptive Embedding Integration"
4. **InsightFace** (2018): Face analysis toolkit
5. **Pix2Pix** (2017): "Image-to-Image Translation with Conditional GANs"

---

## â“ **FAQs**

**Q: Can I use different face detection models?**  
A: Yes! Replace SCRFD with RetinaFace, MTCNN, etc. Just modify `Face_detect_crop` class.

**Q: Which model is most important?**  
A: AEI-Net (MODEL 4). It does the actual face swapping. Others are helpers.

**Q: Can I train my own models?**  
A: Yes, but you need:
- Large face dataset (10K+ images)
- GPU cluster (days of training)
- Training code (available in `train.py`)

**Q: Why so many models?**  
A: Each solves a specific problem. You can't do realistic face swap with just one model.

**Q: Which model determines the quality?**  
A: Primarily AEI-Net (Generator), but all contribute:
- Bad detection = wrong face crop = bad swap
- Bad alignment = misaligned features = artifacts
- Bad blending = visible seams = uncanny valley

---

**Summary**: You're using a sophisticated 5-model pipeline where each model has a specific role. The core magic happens in AEI-Net (MODEL 4), which adaptively blends source identity with target attributes using AAD blocks.

